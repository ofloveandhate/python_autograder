import pandas as pd
import os
from os.path import join

# first, function definitions.



def get_students():
	"""
	reads the file 'student_list.csv', so that it can be used to seed the data sets
	"""

	students = pd.read_csv("_autograding/student_list.csv")
	# students.set_index('student_id')
	students.infer_objects()
	return students


def process_filename(filename):
	"""
	gets the student's name, id number, and the file number from the name of a file
	"""
	a = filename.split('_')
	d = dict()
	d['name'] = a[0]
	d['student_id'] = a[1] if a[1] != "LATE" else a[2]
	d['file_number'] = a[2] if a[1] != "LATE" else a[3]

	return d


def get_num_passes(filepath):
	"""
	computes the number of passed tests by reading the top lines of the checker output,
	namely by counting the number of .'s

	this number will be incorrect if there's screenout in the middle of the string
	this can only be prevented by capturing all warnings and printing during all tests,
	which makes the test output significantly less useful.
	"""
	with open(filepath,'r') as file:
		data = file.read()


	as_lines = data.split('\n')

	topline = as_lines.pop(0)

	while len(set(topline)-set('.EF'))>0:
		if 'Warning' in topline:
			as_lines.pop(0) # the next line is a follow-on to the warning we're skipping
			topline = as_lines.pop(0)
		else:
			topline = as_lines.pop(0)

	num_fails = len(topline) - topline.count('.')
	return topline.count('.'), topline




def tests_to_feedback(filename):
	"""
	feed this function a filename, read from a unittest file
	"""
	with open(filename,'r') as file:
		raw_data = file.read()
		data = raw_data.split('\n')

	if 'Traceback' in data[0]:
		return "Your file has a critical error in it, such that the autograder was not able to execute your code, and thus was unable to run any tests:\n\n"+raw_data

	feedback = ''
	topline = data.pop(0)
	num_warnings = 0
	num_weird = 0

	found_warnings = ''
	found_weird = ''
	while len(set(topline)-set('.EF'))>0:

		if 'Warning' in topline:
			if num_warnings == 0:
				found_warnings = found_warnings+'### Detected warnings your code generated:'
			found_warnings = found_warnings+"\n\n===> Your code has a warning in it!!!  Don't ignore warnings!!!!\n\n"+topline+'\n'+data.pop(0)
			topline = data.pop(0)
			num_warnings = num_warnings+1
		else:
			if num_weird==0:
				found_weird=found_weird+'found some unexpected lines from the checker.'
			found_weird = found_weird+topline
			topline = data.pop(0)

	feedback = found_warnings + ("\n\n" if num_warnings>0 else "") + found_weird + ("\n\n" if num_weird>0 else "")
	num_fails = len(topline) - topline.count('.')

	encountered_fails = 0
	linenum=0
	while linenum < len(data):
		line = data[linenum]
		if line.startswith('FAIL: ') or line.startswith('ERROR: '):
			encountered_fails = encountered_fails+1

			#if encountered_fails==1:
			#	feedback = feedback+'### Autograder tests failed in this autochecker file:\n\n'
			d = line.split(' ')
			test_name = d[1]
			feedback = feedback+f'{encountered_fails}: {test_name}\n'
			linenum+=5
			feedback = feedback+f'\t --reason: failed {data[linenum]}\n\n'
		linenum+=1


	if encountered_fails!=num_fails:
		raise RuntimeError(f'mismatched number of not-passed tests, expected {num_fails}, found {encountered_fails}, in {filename}')
	return feedback




def collect_num_passes(path):
	"""
	constructs a data frame, holding the number of passes, etc.
	scans all .txt files in the folder, which were presumably
	generated by running a checker against students' code.
	"""

	name = []
	student_id = []
	file_number = []
	num_passes = []
	success_string = []
	feedback = []
	# canvas_name = []
	# section = []

	files = os.listdir(path)
	for f in files:
		if f.endswith('.txt'):
			d = process_filename(f)
			name.append(d['name'])
			student_id.append(int(d['student_id']))
			file_number.append(int(d['file_number']))
			q = get_num_passes(join(path,f))
			num_passes.append(q[0])
			success_string.append(q[1])
			feedback.append(tests_to_feedback(join(path,f)))

	df = pd.DataFrame({'name':name, "student_id":student_id,
		"file_number":file_number, "num_passes":num_passes,
		"success_string":success_string, 'feedback':feedback})

	df['num_tests_ran'] = df['success_string'].map(lambda x: len(x))
	df['percent_pass'] = df['num_passes']/df['num_tests_ran']
	return df



def combine_feedback(row):
	"""
	a function to apply, to merge two feedbacks -- from pre and post.
	"""

	if row['percent_pass_pre']==1 and row['percent_pass_post']==1:
		return "\nNice work, all automatically run tests passed!\n\n# Manual grading comments:\n\n"

	feedback = '# Manual grading comments:\n\n\n\n'
	feedback = feedback + '# Automatically generated feedback on your last submitted code'
	if row['percent_pass_pre']<1:
		feedback = feedback + '\n\n## While grading, we detected that the following issues from the provided assignment checker file:\n\n'
		feedback = feedback + row['feedback_pre']

	if row['percent_pass_post']<1:
		feedback = feedback + '\n\n## While grading, we detected that the following issues from an instructor-only checker file:\n\n'
		feedback = feedback + row['feedback_post']

	# put line of code indicating autograder raw score here
	return feedback



def format_feedback(row):
	"""
	a function to apply to the data frame, adding some stuff to end of the feedback
	"""
	val = '\n{}\n\n{}\n'.format(row['canvas_name'],row['feedback_combined'])
	val = val + '\nEnd of code feedback for {}.\n\n'.format(row['canvas_name'])
	val = val + '\n**********************\nNext student\n===================\n'
	return val


def save_feedback(feedback, filename):
	"""
	saves the feedback data frame to a file named `filename`
	"""

	with open(filename,'w') as file:  # i'm not sure this needs to be here.  can remove?
		formatted = feedback.apply(format_feedback,axis=1)
		formatted.to_csv(filename,index=False,header=False)


def reformat_grades_csv(fname):
	"""
	reads in an outputted csv of grades,
	and adjusts it so that it's nicely aligned.
	"""
	def split_into_cols(r):
		r = r.split('"',2)
		return r[0].split(',')[:-1] + [('"'+r[1]+'"')] + r[2].split(',')[1:]


	def compute_column_sizes(as_lines):
		d = [split_into_cols(ell) for ell in as_lines[1:]] # [1:] to skip the header row
		temp_df = pd.DataFrame(data=d)
		return temp_df.applymap(lambda x: len(x)).max()

	def format_line(sizes, row):
		r = row.split('"',2)
		r = r[0].split(',')[:-1] + [('"'+r[1]+'"')] + r[2].split(',')[1:]
		d = r[0]
		s = ' '*(3-len(d))+d+',' # that 3 is the width of the index column

		for ii in range(1,len(r)):

			assert(len(sizes)==len(r)) # just cuz, let's keep it sane

			col_len = sizes[ii]+2 # 2 is the padding
			d = r[ii]

			c = ' '*(col_len-len(d)) + d +','
			s = s+c
		return s+'\n'

	with open(fname,'r') as f:
		as_lines = f.read().split('\n')
		as_lines = [ell for ell in as_lines if ell] # drop empty lines

	# format each line that's not the header line
	formatted = [as_lines[0]+',\n']+[ format_line(compute_column_sizes(as_lines),ell) for ell in as_lines[1:] if ell ]

	# write the file back to disk.
	with open(fname,'w') as f:
		for line in formatted:
			f.write(line)


def write_grades_to_csv(grades):
	"""
	writes `grades` data frame to file, making some adjustments
	to the data frame first.
	"""
	# grades.set_index('student_id')

	grades.drop(['feedback_pre','feedback_post','feedback_combined','name_pre','name_post','file_number_post'],inplace=True,axis=1)

	grades = grades.merge(students, left_on = ['student_id'], right_on =['student_id'], how = 'right')

	grades.sort_values(by=['section','canvas_name'], inplace=True)

	# round, because all those decimal places were not helpful at all.
	grades[['percent_pass_post','percent_pass_pre','autograde_score']] = grades[['percent_pass_post','percent_pass_pre','autograde_score']].round(4)

	fname = '_autograding/checker_results.csv'
	grades.to_csv(fname)

	reformat_grades_csv(fname)


### end function definitions





##### begin actual running of code

if __name__=="__main__":
	students = get_students()


	presub = collect_num_passes('_autograding/pre_checker_results')
	postsub = collect_num_passes('_autograding/post_checker_results')




	grades = presub.merge(postsub, on=('student_id'), suffixes=['_pre','_post'])

	grades['feedback_combined'] = grades.apply(combine_feedback, axis=1)



	# grab only the desired columns for the feedback, to write to csv
	# keep the feedback separate from the grades
	feedback = grades[['name_pre','feedback_combined','student_id']]
	feedback.columns = ['name', 'feedback_combined','student_id']
	feedback = feedback.merge(students, left_on = ['student_id'], right_on =['student_id'], how = 'right')
	import csv
	for sec in feedback.section.unique():
		this_sec = feedback[feedback.section==sec].drop(['section'],axis=1)
		sec_name = sec.strip().replace(' ','_')
		save_feedback(this_sec, f'_autograding/code_feedback_{sec_name}.md')



	grades['autograde_score'] = 45*grades['percent_pass_pre'] + 45*grades['percent_pass_post']

	write_grades_to_csv(grades)
